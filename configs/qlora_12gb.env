# BASE_MODEL is set by protocol.env or run_training_protocol.py
OUTPUT_DIR=artifacts/openclaw-lora-12gb
LEARNING_RATE=2e-4
NUM_EPOCHS=3
PER_DEVICE_TRAIN_BATCH_SIZE=2
GRADIENT_ACCUMULATION_STEPS=16
MAX_SEQ_LENGTH=1536
MAX_NEW_TOKENS=768
LORA_R=16
LORA_ALPHA=16
LORA_DROPOUT=0.05
WARMUP_RATIO=0.03
LR_SCHEDULER_TYPE=cosine
EVAL_STEPS=200
SAVE_STEPS=200
EARLY_STOPPING_PATIENCE=3
LOAD_BEST_MODEL_AT_END=true
MAX_STEPS=-1
# Optional multi-disk shard lists (comma-separated):
# TRAIN_FILES=/mnt/disk1/train_a.jsonl,/mnt/disk2/train_b.jsonl
# VAL_FILES=/mnt/disk1/val_a.jsonl,/mnt/disk2/val_b.jsonl
DATASET_MAP_NUM_PROC=4
DATALOADER_NUM_WORKERS=4
DATALOADER_PREFETCH_FACTOR=2
DATALOADER_PERSISTENT_WORKERS=true
DATALOADER_PIN_MEMORY=true
