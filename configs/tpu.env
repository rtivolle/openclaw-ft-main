# TPU-optimised config for OpenClaw training
# Use with: TRAIN_CONFIG=configs/tpu.env

# --- Model ---
# BASE_MODEL is set in protocol.env

# --- Training hyperparameters (TPU-tuned) ---
PER_DEVICE_TRAIN_BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=8
MAX_SEQ_LENGTH=2048
NUM_EPOCHS=3
LEARNING_RATE=2e-4
WARMUP_RATIO=0.03
LR_SCHEDULER_TYPE=cosine
MAX_GRAD_NORM=1.0
NEFTUNE_NOISE_ALPHA=5.0
EVAL_STEPS=200
SAVE_STEPS=200
EARLY_STOPPING_PATIENCE=3

# --- LoRA ---
LORA_R=16
LORA_ALPHA=16
# Zero dropout avoids XLA recompilation from dynamic masking
LORA_DROPOUT=0.0

# --- Optimizer ---
# adafactor: memory-efficient, TPU-native (no 8-bit state needed)
OPTIMIZER=adafactor

# --- Workers ---
DATALOADER_NUM_WORKERS=4
DATASET_MAP_NUM_PROC=8
